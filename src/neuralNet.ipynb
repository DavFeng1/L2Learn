{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "613e6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7d7b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "testDataFrame = pd.read_csv(\"./datasets/mnist/mnistTest.csv\")\n",
    "trainDataFrame = pd.read_csv(\"./datasets/mnist/mnistTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d1a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy arrays\n",
    "testData = np.array(testDataFrame)\n",
    "trainData = np.array(trainDataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c10c3",
   "metadata": {},
   "source": [
    "# Forward Propogation\n",
    "Begin with vector input $v \\in \\mathbb{R}^n$ where $n = 28 * 28 = 784$ \n",
    "\n",
    "\n",
    "The value at the hidden layer before regularization\n",
    "$$\n",
    "  \\alpha^0_i =\n",
    "      \\sum_{j} w^0_{ji} v_j + b^0_i\n",
    "$$\n",
    "\n",
    "where $w_{ij}$ is the weight of neuron $i$ going into neuron $j$ and $b$ is the bias. Let\n",
    " $\\sigma$ be  some non linear activation function\n",
    "\n",
    "The value at the output layer\n",
    "$$\n",
    "  \\alpha^1_i = \\sum_j w^1_{ji} \\sigma (\\alpha^0_j) + b^1_i\n",
    "$$\n",
    "\n",
    "Then we apply a softmax function to get our prediction probabilities at the output layer\n",
    "$$\n",
    "  p_i = \\text{softmax} (\\alpha^1_i)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24669",
   "metadata": {},
   "source": [
    "# Back Propogation\n",
    "\n",
    "Define the error function as the squared difference between prediction and labelled value. Let $l_i$ define the label for input $i$\n",
    "$$\n",
    "  E(w^0, w^1) = \\frac{1}{2} \\sum_i (l_i - p_i)^2\n",
    "$$\n",
    "We want to minimize $E(w^0, w^1)$. Handle the $w^0$ and $w^1$ cases separately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5a6bd3",
   "metadata": {},
   "source": [
    "\n",
    "### Hidden layer weights $w^1$\n",
    "$$\n",
    "  \\frac{\\partial E}{\\partial w^1_{ij}} = \\sum_k \\frac{\\partial E}{\\partial p_k} \\frac{\\partial p_k}{\\partial \\alpha^1_k} \\frac{\\partial \\alpha^1_k}{w^1_{ij}}\n",
    "$$\n",
    "\n",
    "Calculate each factor\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial E}{\\partial p_k} &= \\frac{1}{2} (l_k - p_k) * (-1) \\\\\n",
    "  &= (p_k - l_k) \\\\\n",
    "  \\\\\n",
    "  \\frac{\\partial p_k}{\\partial \\alpha^1_k} \n",
    "    \n",
    "    &= \\frac{\\partial}{\\partial \\alpha^1_k} \n",
    "        \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "  \\\\\n",
    "  & = e^{\\alpha^1_k} \n",
    "    \\bigg ( \n",
    "      \\frac{ e^{\\alpha^1_k} }{ \n",
    "        \\big( \n",
    "          \\sum_l e^{ \\alpha^1_k } \n",
    "        \\big)^2} +\n",
    "      \\frac{1}{ \\sum_l e^{ \\alpha^1_l } } \n",
    "    \\bigg ) \n",
    "  \\\\\n",
    "  & = \\frac{ e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l} } \n",
    "    \\bigg ( \n",
    "      1 + \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "    \\bigg )\n",
    "  \\\\\n",
    "  & = p_k ( 1 + p_k)\n",
    "  \n",
    "\\end{aligned}\n",
    "$$\n",
    "Finally the derivative wrt the weight\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{ \\partial \\alpha^1_k }{ \\partial w^1_{ij} } \n",
    "    &= \n",
    "    \\frac{\\partial}{\\partial w^1_{ij}} \n",
    "    \\bigg( \n",
    "      \\sum_l w^1_{lk} \\alpha^0_l + b^1_k\n",
    "    \\bigg)\n",
    "  \\\\\n",
    "&= \\sum_l \\delta_{kj} \\delta_{il} \\alpha_l = \\delta_{kj} \\alpha_i\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\delta$ denotes the Kroneckor delta.\n",
    "\n",
    "We increment the hidden layer output weights proportional to the derivative of the error where $\\epsilon$ is some proportionality factor\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^1_{ij} &= - \\epsilon \\sum_k p_k(p_k - l_k)(1 + p_k) \\delta_{kj} \\alpha_i\n",
    "  \\\\\n",
    "  &= - \\epsilon p_j(p_j - l_j)(1 + p_j) \\alpha_i \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e554ce5",
   "metadata": {},
   "source": [
    "### Input layer weights $w^0$\n",
    "\n",
    "$$\n",
    "  \\frac{ \\partial E }{ \\partial w^0_{ij} } = \n",
    "  \\sum_k\n",
    "    \\frac{ \\partial E }{ \\partial p_k } \n",
    "    \\frac{ \\partial p_k }{ \\partial \\alpha^1_k } \n",
    "    \\frac{ \\partial \\alpha^1_k }{ w^0_{ij} }\n",
    "$$\n",
    "\n",
    "Only the last term is different. Keeping $\\sigma$ generic:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\alpha^1_k}{\\partial w^0_{ij}} \n",
    "  \n",
    "    &= \\frac{ \\partial }{ \\partial w^0_{ij} } \n",
    "      \\bigg (\n",
    "        \\sum_l w^1_{lk} \\sigma (\\alpha^0_l) + b^1_k\n",
    "      \\bigg)\n",
    "    \\\\\n",
    "\n",
    "     &= \\sum_l \n",
    "      w^1_{lk} \n",
    "      \\cdot\n",
    "      \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "        \\sigma \n",
    "          \\big( \n",
    "            \\alpha^0_l\n",
    "          \\big ) \n",
    "    \\\\\n",
    "\n",
    "    &= \\sum_l \n",
    "        w^1_{lk} \n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "          \\bigg ( \n",
    "            \\sum_{m} w^0_{ml} v_m + b^0_l\n",
    "          \\bigg )\n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\sum_m\n",
    "        \\delta_{im} \\delta_{jl} v_m \n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\delta_{jl} v_i\n",
    "    \\\\\n",
    "    &= w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "        \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the previously calculated terms we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^0_{ij}\n",
    "  &= \n",
    "  - \\epsilon \\cdot \\sum_k\n",
    "    (p_k - l_k)\n",
    "    p_k\n",
    "    (1 + p_k) \n",
    "    \\cdot \n",
    "    w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "  \\\\\n",
    "  &= - \\epsilon \n",
    "    \\cdot \\sigma'(\\alpha^0_j) \n",
    "    v_i \n",
    "    \\sum_k\n",
    "     (p_k - l_k)\n",
    "      p_k\n",
    "      (1 + p_k) \n",
    "      w^1_{jk}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "37e8fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our training variables\n",
    "\n",
    "# Epsilon is our step size in gradient descent\n",
    "epsilon = 0.01\n",
    "\n",
    "# The labels for the data l\n",
    "labels = trainData[:,0]\n",
    "\n",
    "# The input vector\n",
    "input = trainData[:,1:]\n",
    "\n",
    "# Dimension of the input space (28 * 28 = 784 pixels)\n",
    "n = 784\n",
    "\n",
    "# Dimension of hidden layer ( 4 nodes )\n",
    "m = 4\n",
    "\n",
    "# Dimension of output ( 10 possible digits to classify )\n",
    "l = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4c629",
   "metadata": {},
   "source": [
    "### Initialize the weights $w^1_{ij}$ and $ w^2_{kl}$\n",
    "\n",
    "- $w^0_{ij}$ will be a linear map $w^0: \\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "\n",
    "- $w^1_{kl}$ will be a linear map $w^1: \\mathbb{R}^m \\to \\mathbb{R}^l$ \n",
    "\n",
    "Where\n",
    "- $n$ is the dimension of the input \n",
    "- $m$ is the dimension of the hidden layer\n",
    "- $l$ is the dimension of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "910d1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights for the input and the hidden layer\n",
    "w0 = np.random.random([n, m])\n",
    "w1 = np.random.random([m, l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f313792",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f804f466ef66c655e337d369de345d665385bf381973db70085cd28ea50c1c24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
