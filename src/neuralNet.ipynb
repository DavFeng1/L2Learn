{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613e6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7d7b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "testDataFrame = pd.read_csv(\"./datasets/mnist/mnistTest.csv\")\n",
    "trainDataFrame = pd.read_csv(\"./datasets/mnist/mnistTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d1a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy arrays\n",
    "testData = np.array(testDataFrame)\n",
    "trainData = np.array(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "46ebce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "def sigmoid(x):\n",
    "  return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "  return np.exp(-x) / ( 1 + np.exp(-x)) ** 2\n",
    "\n",
    "def ReLU(Z):\n",
    "  return np.maximum(Z, 0)\n",
    "\n",
    "def ReluPrime(Z):\n",
    "  return Z > 0\n",
    "\n",
    "def softmax(Z):\n",
    "  A = np.exp(Z) / sum(np.exp(Z))\n",
    "  return A\n",
    "\n",
    "def error(l, p):\n",
    "  if (len(p) != len(l)):\n",
    "    raise ValueError(\"Lengths must be the same\")\n",
    "\n",
    "  return np.sum([0.5 * (p[i] - l[i])**2 for i in range(len(p))])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4c629",
   "metadata": {},
   "source": [
    "### Initialize the weights $w^1_{ij}$ and $ w^2_{kl}$\n",
    "\n",
    "- $w^0_{ij}$ will be a linear map $w^0: \\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "\n",
    "- $w^1_{kl}$ will be a linear map $w^1: \\mathbb{R}^m \\to \\mathbb{R}^l$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "37e8fd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (60000, 784)\n",
      "Y shape (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Epsilon is our step size in gradient descent\n",
    "epsilon = 0.1\n",
    "# The labels for the data l\n",
    "labelsRaw = trainData[:,0]\n",
    "# One hot encode labels\n",
    "Y = np.array([[1 if label == i else 0 for i in range(10)] for label in labelsRaw])\n",
    "# The input vector\n",
    "X = trainData[:,1:]\n",
    "# Dimension of the input space (28 * 28 = 784 pixels)\n",
    "n = 784\n",
    "# Dimension of hidden layer ( 4 nodes )\n",
    "m = 4\n",
    "# Dimension of output ( 10 possible digits to classify )\n",
    "l = 10\n",
    "# Size of training data\n",
    "k = 60000\n",
    "\n",
    "print(\"X shape\", X.shape)\n",
    "print(\"Y shape\", Y.shape)\n",
    "\n",
    "def initializeParameters():\n",
    "  # Initialize weights for the input and the hidden layer\n",
    "  # Between -0.5 and 0.5 since we are using ReLU as activation function\n",
    "  W0 = np.random.rand(n, m) - 0.5\n",
    "  W1 = np.random.rand(m, l) - 0.5\n",
    "\n",
    "  # Initialize biases\n",
    "  b0 = np.random.rand(m) - 0.5\n",
    "  b1 = np.random.rand(l) - 0.5\n",
    "\n",
    "  return W0, b0, W1, b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0aa40",
   "metadata": {},
   "source": [
    "#### Update Bias $b^0$\n",
    "<details>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\alpha^1_k}{\\partial b^0_i} &= \\frac{\\partial}{\\partial b^0_i}\n",
    "  \\bigg (\n",
    "      \\sum_l w^1_{lk} \\sigma ( \\alpha^0_l ) + b^1_k\n",
    "  \\bigg )\n",
    "  \\\\\n",
    "  & = \\sum_l w^1_{lk} \\sigma'( \\alpha^0_l )  \\delta_{il}\n",
    "  \\\\\n",
    "  & =  w^1_{ik} \\sigma'( \\alpha^0_i )\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\Delta b^0_i = - \\epsilon \\cdot  \\sigma'(\\alpha^0_i) \\cdot \\sum_k (p_k - l_k)p_k (1 + p_k) \\cdot w^1_{ik}\n",
    "\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a2755",
   "metadata": {},
   "source": [
    "#### Update Bias $b^1$\n",
    "<details>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\alpha^1_k}{\\partial b^1_i} &= \\frac{\\partial}{\\partial b^1_i}\n",
    "  \\bigg (\n",
    "      \\sum_l w^1_{lk} \\sigma ( \\alpha^0_l ) + b^1_k\n",
    "  \\bigg )\n",
    "  \\\\\n",
    "  & =  \\delta_{ki}\n",
    "\\end{align}\n",
    "$$\n",
    "$$\n",
    "\\Delta b^0_i = - \\epsilon  (p_i - l_i)p_i (1 + p_i)\n",
    "\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c20766a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardProp(W0, W1, b0, b1, X_i):\n",
    "  # Z is value before activation function is applied\n",
    "  A0 = W0.T.dot(X_i) + b0\n",
    "  Z0 = ReLU(A0)\n",
    "  Z1 = W1.T.dot(Z0) + b1\n",
    "  P = softmax(Z1)\n",
    "  return A0, P\n",
    "\n",
    "def backProp(W1, P, X_i, Y_i, A0):\n",
    "\n",
    "  def z(k):\n",
    "    return (P[k] - Y_i[k]) * P[k] * (1 + P[k])\n",
    "\n",
    "  dW0 = np.array([[ReluPrime(A0[j]) \n",
    "    * X_i[j] \n",
    "    * np.sum([z(k) * W1[j][k] for k in range(l)]) \n",
    "        for i in range(n)] \n",
    "          for j in range(m)])\n",
    "\n",
    "  db0 = np.array([\n",
    "    ReluPrime(A0[i]) * \n",
    "    np.sum([z(k) * W1[i][k] for k in range(l)]) \n",
    "      for i in range(m)])\n",
    "\n",
    "  dW1 = np.array([[ReluPrime(A0[i]) \n",
    "    * z(j) \n",
    "    * A0[i] \n",
    "      for i in range(m)] \n",
    "        for j in range(l)])\n",
    "\n",
    "  db1 = np.array([z(k) for k in range(l)])\n",
    "  return dW0, db0, dW1, db1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "fef4bfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(P):\n",
    "    return np.argmax(P, 0)\n",
    "\n",
    "def getAccuracy(p, Y):\n",
    "  return np.sum(p == Y) / Y.size\n",
    "\n",
    "def updateParams(W0, W1, b0, b1, dW0, dW1, db0, db1, epsilon):\n",
    "  W0 = W0 - epsilon * dW0.T\n",
    "  b0 = b0 - epsilon * db0\n",
    "  W1 = W1 - epsilon * dW1.T\n",
    "  b1 = b1 - epsilon * db1\n",
    "\n",
    "  return W0, b0, W1, b1\n",
    "\n",
    "#  One iteration\n",
    "def gradientDescent(X, Y, epsilon, iterations):\n",
    "  \n",
    "  W0, b0, W1, b1 = initializeParameters()\n",
    "\n",
    "  for i in range(iterations):\n",
    "    A0, P = forwardProp(W0, W1, b0, b1, X[i])\n",
    "\n",
    "    dW0, db0, dW1, db1 = backProp(W1, P, X[i], Y[i], A0)\n",
    "\n",
    "    W0, b0, W1, b1 = updateParams(W0, W1, b0, b1, dW0, dW1, db0, db1, epsilon)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "      print(\"Iteration: \", i)\n",
    "      print(\"Accuracy: \", getAccuracy(getPredictions(P), Y))\n",
    "\n",
    "  return W0, b0, W1, b1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7f26e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Accuracy:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dfeng/documents/L2Learn/venv/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/dfeng/documents/L2Learn/venv/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  5\n",
      "Accuracy:  0.0\n",
      "Iteration:  10\n",
      "Accuracy:  0.9\n",
      "Iteration:  15\n",
      "Accuracy:  0.9\n",
      "Iteration:  20\n",
      "Accuracy:  0.9\n",
      "Iteration:  25\n",
      "Accuracy:  0.9\n",
      "Iteration:  30\n",
      "Accuracy:  0.9\n",
      "Iteration:  35\n",
      "Accuracy:  0.9\n",
      "Iteration:  40\n",
      "Accuracy:  0.9\n",
      "Iteration:  45\n",
      "Accuracy:  0.9\n",
      "Iteration:  50\n",
      "Accuracy:  0.9\n",
      "Iteration:  55\n",
      "Accuracy:  0.9\n",
      "Iteration:  60\n",
      "Accuracy:  0.9\n",
      "Iteration:  65\n",
      "Accuracy:  0.9\n",
      "Iteration:  70\n",
      "Accuracy:  0.9\n",
      "Iteration:  75\n",
      "Accuracy:  0.9\n",
      "Iteration:  80\n",
      "Accuracy:  0.9\n",
      "Iteration:  85\n",
      "Accuracy:  0.9\n",
      "Iteration:  90\n",
      "Accuracy:  0.9\n",
      "Iteration:  95\n",
      "Accuracy:  0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]]),\n",
       " array([nan, nan, nan, nan]),\n",
       " array([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]]),\n",
       " array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]))"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientDescent( X, Y, epsilon, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0b63adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0 shape (784, 4)\n",
      "w1 shape (4, 10)\n",
      "b0 shape (4,)\n",
      "b1 shape (10,)\n",
      "A0:  (4,)\n",
      "P:  (10,)\n",
      "dW0:  (4, 784)\n",
      "db0:  (4,)\n",
      "dW1:  (10, 4)\n",
      "db1:  (10,)\n"
     ]
    }
   ],
   "source": [
    "W0, b0, W1, b1 = initializeParameters()\n",
    "print(\"w0 shape\", W0.shape)\n",
    "print(\"w1 shape\", W1.shape)\n",
    "print(\"b0 shape\", b0.shape)\n",
    "print(\"b1 shape\", b1.shape)\n",
    "\n",
    "A0, P = forwardProp(W0, W1, b0, b1, X[0])\n",
    "\n",
    "print(\"A0: \", A0.shape)\n",
    "print(\"P: \", P.shape)\n",
    "\n",
    "\n",
    "dW0, db0, dW1, db1 = backProp(W1, P, X[0], Y[0], A0)\n",
    "\n",
    "print(\"dW0: \", dW0.shape)\n",
    "print(\"db0: \", db0.shape)\n",
    "print(\"dW1: \", dW1.shape)\n",
    "print(\"db1: \", db1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24669",
   "metadata": {},
   "source": [
    "### Forward Propogation\n",
    "<details>\n",
    "\n",
    "Begin with vector input $v \\in \\mathbb{R}^n$ where $n = 28 * 28 = 784$. The value at the hidden layer before regularization\n",
    "\n",
    "$$\n",
    "  \\alpha^0_i =\n",
    "      \\sum_{j} w^0_{ji} v_j + b^0_i\n",
    "$$\n",
    "\n",
    "where $w_{ij}$ is the weight of neuron $i$ going into neuron $j$ and $b$ is the bias. Let\n",
    " $\\sigma$ be  some non linear activation function\n",
    "\n",
    "The value at the output layer\n",
    "$$\n",
    "  \\alpha^1_i = \\sum_j w^1_{ji} \\sigma (\\alpha^0_j) + b^1_i\n",
    "$$\n",
    "\n",
    "Then we apply a softmax function to get our prediction probabilities at the output layer\n",
    "$$\n",
    "  p_i = \\text{softmax} (\\alpha^1_i)\n",
    "$$\n",
    "\n",
    "###  Back Propogation\n",
    "\n",
    "Define the error function as the squared difference between prediction and labelled value. Let $l_i$ define the label for input $i$\n",
    "$$\n",
    "  E(w^0, w^1) = \\frac{1}{2} \\sum_i (l_i - p_i)^2\n",
    "$$\n",
    "We want to minimize $E(w^0, w^1)$. Handle the $w^0$ and $w^1$ cases separately.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e554ce5",
   "metadata": {},
   "source": [
    "### Hidden layer weights $w^1$\n",
    "<details>\n",
    "\n",
    "$$\n",
    "  \\frac{\\partial E}{\\partial w^1_{ij}} = \\sum_k \\frac{\\partial E}{\\partial p_k} \\frac{\\partial p_k}{\\partial \\alpha^1_k} \\frac{\\partial \\alpha^1_k}{w^1_{ij}}\n",
    "$$\n",
    "\n",
    "Calculate each factor\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial E}{\\partial p_k} &= \\frac{1}{2} (l_k - p_k) * (-1) \\\\\n",
    "  &= (p_k - l_k) \\\\\n",
    "  \\\\\n",
    "  \\frac{\\partial p_k}{\\partial \\alpha^1_k} \n",
    "    \n",
    "    &= \\frac{\\partial}{\\partial \\alpha^1_k} \n",
    "        \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "  \\\\\n",
    "  & = e^{\\alpha^1_k} \n",
    "    \\bigg ( \n",
    "      \\frac{ e^{\\alpha^1_k} }{ \n",
    "        \\big( \n",
    "          \\sum_l e^{ \\alpha^1_k } \n",
    "        \\big)^2} +\n",
    "      \\frac{1}{ \\sum_l e^{ \\alpha^1_l } } \n",
    "    \\bigg ) \n",
    "  \\\\\n",
    "  & = \\frac{ e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l} } \n",
    "    \\bigg ( \n",
    "      1 + \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "    \\bigg )\n",
    "  \\\\\n",
    "  & = p_k ( 1 + p_k)\n",
    "  \n",
    "\\end{aligned}\n",
    "$$\n",
    "Finally the derivative wrt the weight\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{ \\partial \\alpha^1_k }{ \\partial w^1_{ij} } \n",
    "    &= \n",
    "    \\frac{\\partial}{\\partial w^1_{ij}} \n",
    "    \\bigg( \n",
    "      \\sum_l w^1_{lk} \\sigma ( \\alpha^0_l ) + b^1_k\n",
    "    \\bigg)\n",
    "  \\\\\n",
    "&= \\sum_l \\sigma ' ( \\alpha^0_l ) \\cdot \\delta_{kj} \\delta_{il} \\alpha_l = \\delta_{kj} \\sigma ' ( \\alpha^0_i ) \\alpha_i\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\delta$ denotes the Kroneckor delta. Increment the hidden layer output weights proportional to the derivative of the error where $\\epsilon$ is some proportionality factor\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^1_{ij} &= - \\epsilon \\sum_k p_k(p_k - l_k)(1 + p_k) \\delta_{kj} \\alpha^0_i \\sigma ' ( \\alpha^0_i )\n",
    "  \\\\\n",
    "  &= - \\epsilon p_j(p_j - l_j)(1 + p_j) \\alpha^0_i \\sigma ' ( \\alpha^0_i )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "</details>\n",
    "\n",
    "### Input layer weights $w^0$\n",
    "<details>\n",
    "\n",
    "$$\n",
    "  \\frac{ \\partial E }{ \\partial w^0_{ij} } = \n",
    "  \\sum_k\n",
    "    \\frac{ \\partial E }{ \\partial p_k } \n",
    "    \\frac{ \\partial p_k }{ \\partial \\alpha^1_k } \n",
    "    \\frac{ \\partial \\alpha^1_k }{ w^0_{ij} }\n",
    "$$\n",
    "\n",
    "Only the last term is different. Keeping $\\sigma$ generic:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\alpha^1_k}{\\partial w^0_{ij}} \n",
    "  \n",
    "    &= \\frac{ \\partial }{ \\partial w^0_{ij} } \n",
    "      \\bigg (\n",
    "        \\sum_l w^1_{lk} \\sigma (\\alpha^0_l) + b^1_k\n",
    "      \\bigg)\n",
    "    \\\\\n",
    "\n",
    "     &= \\sum_l \n",
    "      w^1_{lk} \n",
    "      \\cdot\n",
    "      \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "        \\sigma \n",
    "          \\big( \n",
    "            \\alpha^0_l\n",
    "          \\big ) \n",
    "    \\\\\n",
    "\n",
    "    &= \\sum_l \n",
    "        w^1_{lk} \n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "          \\bigg ( \n",
    "            \\sum_{m} w^0_{ml} v_m + b^0_l\n",
    "          \\bigg )\n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\sum_m\n",
    "        \\delta_{im} \\delta_{jl} v_m \n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\delta_{jl} v_i\n",
    "    \\\\\n",
    "    &= w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "        \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the previously calculated terms we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^0_{ij}\n",
    "  &= \n",
    "  - \\epsilon \\cdot \\sum_k\n",
    "    (p_k - l_k)\n",
    "    p_k\n",
    "    (1 + p_k) \n",
    "    \\cdot \n",
    "    w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "  \\\\\n",
    "  &= - \\epsilon \n",
    "    \\cdot \\sigma'(\\alpha^0_j) \n",
    "    v_i \n",
    "    \\sum_k\n",
    "     (p_k - l_k)\n",
    "      p_k\n",
    "      (1 + p_k) \n",
    "      w^1_{jk}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f804f466ef66c655e337d369de345d665385bf381973db70085cd28ea50c1c24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
