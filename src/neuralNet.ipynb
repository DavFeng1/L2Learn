{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "613e6b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7d7b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import datasets\n",
    "testDataFrame = pd.read_csv(\"./datasets/mnist/mnistTest.csv\")\n",
    "trainDataFrame = pd.read_csv(\"./datasets/mnist/mnistTrain.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95d1a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into numpy arrays\n",
    "testData = np.array(testDataFrame)\n",
    "trainData = np.array(trainDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "37e8fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our training variables\n",
    "\n",
    "# Epsilon is our step size in gradient descent\n",
    "epsilon = 0.001\n",
    "\n",
    "# The labels for the data l\n",
    "labelsRaw = trainData[:,0]\n",
    "labels = [[1 if label == i else 0 for i in range(10)] for label in labelsRaw]\n",
    "\n",
    "# The input vector\n",
    "input = trainData[:,1:]\n",
    "\n",
    "# Dimension of the input space (28 * 28 = 784 pixels)\n",
    "n = 784\n",
    "\n",
    "# Dimension of hidden layer ( 4 nodes )\n",
    "m = 4\n",
    "\n",
    "# Dimension of output ( 10 possible digits to classify )\n",
    "l = 10\n",
    "\n",
    "# Size of training data\n",
    "k = 60000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4c629",
   "metadata": {},
   "source": [
    "### Initialize the weights $w^1_{ij}$ and $ w^2_{kl}$\n",
    "\n",
    "- $w^0_{ij}$ will be a linear map $w^0: \\mathbb{R}^n \\to \\mathbb{R}^m$\n",
    "\n",
    "- $w^1_{kl}$ will be a linear map $w^1: \\mathbb{R}^m \\to \\mathbb{R}^l$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "910d1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights for the input and the hidden layer\n",
    "w0 = np.random.random([n, m])\n",
    "w1 = np.random.random([m, l])\n",
    "\n",
    "# Initialize biases\n",
    "b0 = np.random.random([m])\n",
    "b1 = np.random.random([l])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c89b7",
   "metadata": {},
   "source": [
    "### Forward Propogation\n",
    "Begin with vector input $v \\in \\mathbb{R}^n$ where $n = 28 * 28 = 784$. The value at the hidden layer before regularization\n",
    "\n",
    "$$\n",
    "  \\alpha^0_i =\n",
    "      \\sum_{j} w^0_{ji} v_j + b^0_i\n",
    "$$\n",
    "\n",
    "where $w_{ij}$ is the weight of neuron $i$ going into neuron $j$ and $b$ is the bias. Let\n",
    " $\\sigma$ be  some non linear activation function\n",
    "\n",
    "The value at the output layer\n",
    "$$\n",
    "  \\alpha^1_i = \\sum_j w^1_{ji} \\sigma (\\alpha^0_j) + b^1_i\n",
    "$$\n",
    "\n",
    "Then we apply a softmax function to get our prediction probabilities at the output layer\n",
    "$$\n",
    "  p_i = \\text{softmax} (\\alpha^1_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "f43a6f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define activation function and softmax \n",
    "def sigmoid(x):\n",
    "  return 1 / ( 1 + np.exp(-x))\n",
    "\n",
    "def sigmoidPrime(x):\n",
    "  return np.exp(-x) / ( 1 + np.exp(-x)) ** 2\n",
    "\n",
    "def softmax(x):\n",
    "  return np.array(np.exp(x) / np.sum(np.exp(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "b65f4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy implementation\n",
    "def forwardProp(w0, w1, b0, b1, input, i):\n",
    "  # Caluculate hidden layer values\n",
    "  a0 = w0.T.dot(input[i]) + b0\n",
    "\n",
    "  # Apply relu as activation function\n",
    "  z = np.array([sigmoid(x) for x in a0])\n",
    "\n",
    "  # Calculate output layer values\n",
    "  z = w1.T.dot(z) + b1\n",
    "\n",
    "  # apply softmax\n",
    "  return softmax(z), a0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd24669",
   "metadata": {},
   "source": [
    "##  Back Propogation\n",
    "\n",
    "Define the error function as the squared difference between prediction and labelled value. Let $l_i$ define the label for input $i$\n",
    "$$\n",
    "  E(w^0, w^1) = \\frac{1}{2} \\sum_i (l_i - p_i)^2\n",
    "$$\n",
    "We want to minimize $E(w^0, w^1)$. Handle the $w^0$ and $w^1$ cases separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "cca14975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(l, p):\n",
    "  if (len(p) != len(l)):\n",
    "    raise ValueError(\"Lengths must be the same\")\n",
    "\n",
    "  a = [0.5 * (p[i] - l[i])**2 for i in range(len(p))]\n",
    "  return np.sum(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "c20766a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given error and last values \n",
    "def backProp(w0, w1, b0, b1, p, label, a0 ):\n",
    "\n",
    "  def z(k):\n",
    "    return (p[k] - label[k]) * p[k] * (1 + p[k])\n",
    "\n",
    "\n",
    "  dw1 = np.array([[- epsilon \n",
    "          * sigmoidPrime(a0[i]) \n",
    "          * z(j) \n",
    "          * a0[i] \n",
    "            for i in range(m)] \n",
    "              for j in range(l)])\n",
    "\n",
    "  dw2 = np.array([[- epsilon \n",
    "            * sigmoidPrime(a0[j]) \n",
    "            * input[i] \n",
    "            * np.sum([z(k) * w1[j][k] for k in range(l)]) \n",
    "                for i in range(n)] \n",
    "                  for j in range(m)])\n",
    "  print(dw2)    \n",
    "  w1 = w1 + dw1.T\n",
    "  w0 = w0 + dw2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "fef4bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error before:  0.49172405467448543\n",
      "[[[-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  ...\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]]\n",
      "\n",
      " [[-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  ...\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]]\n",
      "\n",
      " [[-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  ...\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]]\n",
      "\n",
      " [[-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  ...\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]\n",
      "  [-0. -0. -0. ... -0. -0. -0.]]]\n",
      "error after:  0.49172405467448543\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  One iteration\n",
    "i = 0\n",
    "p, a0 = forwardProp(w0, w1, b0, b1, input, i)\n",
    "\n",
    "print('error before: ', error(labels[i], p))\n",
    "\n",
    "backProp(w0, w1, b0, b1, p, labels[i], a0)\n",
    "\n",
    "\n",
    "p_new, a0_new = forwardProp(w0, w1, b0, b1, input, i)\n",
    "\n",
    "print('error after: ', error(labels[i], p_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e554ce5",
   "metadata": {},
   "source": [
    "\n",
    "### Hidden layer weights $w^1$\n",
    "$$\n",
    "  \\frac{\\partial E}{\\partial w^1_{ij}} = \\sum_k \\frac{\\partial E}{\\partial p_k} \\frac{\\partial p_k}{\\partial \\alpha^1_k} \\frac{\\partial \\alpha^1_k}{w^1_{ij}}\n",
    "$$\n",
    "\n",
    "Calculate each factor\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\frac{\\partial E}{\\partial p_k} &= \\frac{1}{2} (l_k - p_k) * (-1) \\\\\n",
    "  &= (p_k - l_k) \\\\\n",
    "  \\\\\n",
    "  \\frac{\\partial p_k}{\\partial \\alpha^1_k} \n",
    "    \n",
    "    &= \\frac{\\partial}{\\partial \\alpha^1_k} \n",
    "        \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "  \\\\\n",
    "  & = e^{\\alpha^1_k} \n",
    "    \\bigg ( \n",
    "      \\frac{ e^{\\alpha^1_k} }{ \n",
    "        \\big( \n",
    "          \\sum_l e^{ \\alpha^1_k } \n",
    "        \\big)^2} +\n",
    "      \\frac{1}{ \\sum_l e^{ \\alpha^1_l } } \n",
    "    \\bigg ) \n",
    "  \\\\\n",
    "  & = \\frac{ e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l} } \n",
    "    \\bigg ( \n",
    "      1 + \\frac{e^{\\alpha^1_k}}{\\sum_l e^{\\alpha^1_l}} \n",
    "    \\bigg )\n",
    "  \\\\\n",
    "  & = p_k ( 1 + p_k)\n",
    "  \n",
    "\\end{aligned}\n",
    "$$\n",
    "Finally the derivative wrt the weight\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{ \\partial \\alpha^1_k }{ \\partial w^1_{ij} } \n",
    "    &= \n",
    "    \\frac{\\partial}{\\partial w^1_{ij}} \n",
    "    \\bigg( \n",
    "      \\sum_l w^1_{lk} \\sigma ( \\alpha^0_l ) + b^1_k\n",
    "    \\bigg)\n",
    "  \\\\\n",
    "&= \\sum_l \\sigma ' ( \\alpha^0_l ) \\cdot \\delta_{kj} \\delta_{il} \\alpha_l = \\delta_{kj} \\sigma ' ( \\alpha^0_i ) \\alpha_i\n",
    "\\end{align}\n",
    "$$\n",
    "Where $\\delta$ denotes the Kroneckor delta. Increment the hidden layer output weights proportional to the derivative of the error where $\\epsilon$ is some proportionality factor\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^1_{ij} &= - \\epsilon \\sum_k p_k(p_k - l_k)(1 + p_k) \\delta_{kj} \\alpha^0_i \\sigma ' ( \\alpha^0_i )\n",
    "  \\\\\n",
    "  &= - \\epsilon p_j(p_j - l_j)(1 + p_j) \\alpha^0_i \\sigma ' ( \\alpha^0_i )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### Input layer weights $w^0$\n",
    "\n",
    "$$\n",
    "  \\frac{ \\partial E }{ \\partial w^0_{ij} } = \n",
    "  \\sum_k\n",
    "    \\frac{ \\partial E }{ \\partial p_k } \n",
    "    \\frac{ \\partial p_k }{ \\partial \\alpha^1_k } \n",
    "    \\frac{ \\partial \\alpha^1_k }{ w^0_{ij} }\n",
    "$$\n",
    "\n",
    "Only the last term is different. Keeping $\\sigma$ generic:\n",
    "$$\n",
    "\\begin{align}\n",
    "  \\frac{\\partial \\alpha^1_k}{\\partial w^0_{ij}} \n",
    "  \n",
    "    &= \\frac{ \\partial }{ \\partial w^0_{ij} } \n",
    "      \\bigg (\n",
    "        \\sum_l w^1_{lk} \\sigma (\\alpha^0_l) + b^1_k\n",
    "      \\bigg)\n",
    "    \\\\\n",
    "\n",
    "     &= \\sum_l \n",
    "      w^1_{lk} \n",
    "      \\cdot\n",
    "      \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "        \\sigma \n",
    "          \\big( \n",
    "            \\alpha^0_l\n",
    "          \\big ) \n",
    "    \\\\\n",
    "\n",
    "    &= \\sum_l \n",
    "        w^1_{lk} \n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\frac{ \\partial }{ \\partial w^0_{ij} }\n",
    "          \\bigg ( \n",
    "            \\sum_{m} w^0_{ml} v_m + b^0_l\n",
    "          \\bigg )\n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\sum_m\n",
    "        \\delta_{im} \\delta_{jl} v_m \n",
    "    \\\\\n",
    "    &= \\sum_l\n",
    "        w^1_{lk}\n",
    "        \\cdot\n",
    "        \\sigma'(\\alpha^0_l)\n",
    "        \\cdot\n",
    "        \\delta_{jl} v_i\n",
    "    \\\\\n",
    "    &= w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "        \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Combining the previously calculated terms we get\n",
    "$$\n",
    "\\begin{align}\n",
    "\n",
    "  \\Delta w^0_{ij}\n",
    "  &= \n",
    "  - \\epsilon \\cdot \\sum_k\n",
    "    (p_k - l_k)\n",
    "    p_k\n",
    "    (1 + p_k) \n",
    "    \\cdot \n",
    "    w^1_{jk}\n",
    "      \\cdot \n",
    "      \\sigma'(\\alpha^0_j)\n",
    "      \\cdot\n",
    "      v_i\n",
    "  \\\\\n",
    "  &= - \\epsilon \n",
    "    \\cdot \\sigma'(\\alpha^0_j) \n",
    "    v_i \n",
    "    \\sum_k\n",
    "     (p_k - l_k)\n",
    "      p_k\n",
    "      (1 + p_k) \n",
    "      w^1_{jk}\n",
    "\\end{align}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f804f466ef66c655e337d369de345d665385bf381973db70085cd28ea50c1c24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
